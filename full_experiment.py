"""
Full experiment runner for targeted vs exhaustive policy composition.

Outputs a CSV with:
seed, scratch_reward, targeted_reward, exhaustive_reward, decomp_time, targeted_time, exhaustive_time

Definitions:
- scratch_reward: greedy eval of the from-scratch combined policy (states_f/combined/seed_xxxx/q_table_final.npy)
- targeted_reward: compose the top-2 retrieved policies (current search pipeline) using a single query
  decomposition shared across approaches; targeted_time = search/rank/compose (no greedy eval, no decomposition).
- exhaustive_reward: exhaustively compose all candidate pairs, predict reward via regressor on a learned
  embedding (pi2vec successor features over greedy transitions of the combined Q-table), pick best predicted,
  then greedy eval for the reported reward. exhaustive_time = search + all pairwise successor
  training/prediction (no greedy eval, no decomposition).
- decomp_time: time taken for query decomposition (shared across targeted and exhaustive approaches).

Notes:
- Q-table combination is done by elementwise sum (deterministic policies; sum favors shared preference).
- Transitions for embedding are generated by greedy rollout on the combined Q-table (deterministic env).
- Canonical states loaded from data/canonical_states.npy (created by pi2vec_utils).
"""

import argparse
import os
import time

import numpy as np
import pandas as pd

from pi2vec.pi2vec_utils import create_canonical_states, state_to_vector
from pi2vec.train_successor import train_and_save_successor_model
from policy_reusability.env.gridworld import GridWorld
from policy_reusability.my_work.init_gridworld import init_gridworld_rand
from search_faiss_policies import PolicyRetriever


def load_q_table_from_metadata(policy: dict) -> np.ndarray | None:
    q_table = policy.get("q_table")
    if q_table is None:
        return None
    if isinstance(q_table, list):
        q_table = np.array(q_table)
    return q_table


def greedy_eval(
    env: GridWorld, q_table: np.ndarray, max_steps: int | None = None
) -> float:
    env.reset().flatten()
    if max_steps is None:
        max_steps = env.grid_length + env.grid_width + 1
    total_reward = 0.0
    state_index = env.state_to_index(env.agent_position)
    for _ in range(max_steps):
        action = int(np.argmax(q_table[state_index, :]))
        _, reward, done, _ = env.step(action)
        total_reward += reward
        state_index = env.state_to_index(env.agent_position)
        if done:
            break
    return total_reward


def generate_transitions(
    env: GridWorld, q_table: np.ndarray, max_steps: int | None = None
):
    """Greedy rollout transitions (state_vec, next_state_vec) for successor training."""
    env.reset().flatten()
    if max_steps is None:
        max_steps = env.grid_length + env.grid_width + 1
    transitions = []
    state_vec = state_to_vector(env.grid.copy())
    state_index = env.state_to_index(env.agent_position)
    for _ in range(max_steps):
        action = int(np.argmax(q_table[state_index, :]))
        _, reward, done, _ = env.step(action)
        next_vec = state_to_vector(env.grid.copy())
        transitions.append((state_vec, next_vec))
        state_vec = next_vec
        state_index = env.state_to_index(env.agent_position)
        if done:
            break
    return transitions


def combine_q_tables(q1: np.ndarray, q2: np.ndarray) -> np.ndarray:
    # Simple deterministic merge: sum Q-values
    return q1 + q2


def fetch_candidates(
    retriever: PolicyRetriever,
    sub_queries,
    seed: str | None,
    similarity_threshold=0.7,
):
    """Search policies without retry (decomposition is retried instead)."""
    candidates, search_time = decomposed_candidates(
        retriever, sub_queries, seed, similarity_threshold
    )
    return candidates, search_time


def decompose_query_with_retry(
    retriever: PolicyRetriever, query: str, max_attempts: int = 5
):
    """Decompose query with retry logic (up to 5 LLM calls)."""
    total_decomp_time = 0.0
    attempts = 0
    sub_queries = None

    while attempts < max_attempts:
        attempts += 1
        start = time.time()
        try:
            sub_queries = retriever.decompose_query(query)
            elapsed = time.time() - start
            total_decomp_time += elapsed

            # Check if decomposition is valid (has at least 1 sub-query)
            if sub_queries and len(sub_queries) > 0:
                print(
                    f"Decomposed query into {len(sub_queries)} sub-queries in {elapsed:.6f}s (attempt {attempts}/{max_attempts})"
                )
                break
            else:
                if attempts < max_attempts:
                    print(
                        f"Decomposition returned empty/invalid result; retrying (attempt {attempts + 1}/{max_attempts})"
                    )
        except Exception as e:
            elapsed = time.time() - start
            total_decomp_time += elapsed
            if attempts < max_attempts:
                print(
                    f"Decomposition failed: {e}; retrying (attempt {attempts + 1}/{max_attempts})"
                )
            else:
                raise

    if sub_queries is None or len(sub_queries) == 0:
        raise ValueError(f"Failed to decompose query after {max_attempts} attempts")

    print(
        f"Decomposition completed in {total_decomp_time:.6f}s total (reused for targeted/exhaustive)"
    )
    return sub_queries, total_decomp_time


def decomposed_candidates(
    retriever: PolicyRetriever,
    sub_queries,
    seed: str | None,
    similarity_threshold=0.7,
):
    start = time.time()
    candidates = []
    for sq in sub_queries:
        result_dict, timing = retriever.vdb.search_similar_policies(
            sq, policy_seed=seed
        )
        results = result_dict.get("results", [])
        # Apply similarity filter
        results = [r for r in results if r.get("score", 0) > similarity_threshold]
        for r in results:
            emb = r.get("policy_embedding")
            if emb is None:
                continue
            if isinstance(emb, list):
                emb = np.array(emb)
            if retriever.regressor_model:
                pred = float(retriever.regressor_model.predict(emb.reshape(1, -1))[0])
            else:
                pred = 0.0
            r["regressor_score"] = pred
            candidates.append(r)
    return candidates, time.time() - start


def targeted_composition(
    retriever: PolicyRetriever, seed: str | None, sub_queries, canonical_states
):
    """Score candidates from provided sub-queries, take best two overall (sim>0.7 + regressor) and compose."""
    start = time.time()
    candidates, candidate_time = fetch_candidates(retriever, sub_queries, seed)
    print(
        f"[seed {seed}] targeted: candidates={len(candidates)}, search_time={candidate_time:.6f}s (decomp reused)"
    )
    candidates = sorted(
        candidates, key=lambda x: x.get("regressor_score", -1), reverse=True
    )
    if len(candidates) < 2:
        print(
            f"[seed {seed}] targeted: insufficient candidates ({len(candidates)} < 2). "
            f"May need lower similarity threshold or more policies in database."
        )
        return None, None, None, time.time() - start
    p1, p2 = candidates[0], candidates[1]
    q1 = load_q_table_from_metadata(p1)
    q2 = load_q_table_from_metadata(p2)
    if q1 is None or q2 is None:
        missing = []
        if q1 is None:
            missing.append(f"p1={p1.get('policy_name', 'unknown')}")
        if q2 is None:
            missing.append(f"p2={p2.get('policy_name', 'unknown')}")
        print(f"[seed {seed}] targeted: missing Q-tables for {', '.join(missing)}")
        return None, None, None, time.time() - start
    q_combined = combine_q_tables(q1, q2)
    elapsed = time.time() - start
    return q_combined, (p1, p2), (p1.get("dag"), p2.get("dag")), elapsed


def embedding_from_qtable(
    env: GridWorld, q_table: np.ndarray, canonical_states, seed_val: int
):
    transitions = generate_transitions(env, q_table)
    # policy_name needs three parts for train_and_save_successor_model (target_seed_progress)
    policy_name = f"combined_{seed_val:04d}_tmp"
    _, embedding = train_and_save_successor_model(
        policy_name, transitions, canonical_states, epochs=50, show_progress=True
    )
    return embedding


def exhaustive_composition(
    retriever: PolicyRetriever,
    seed: str | None,
    sub_queries,
    canonical_states,
    grid_size: int = 64,
):
    start = time.time()
    candidates, candidate_time = fetch_candidates(retriever, sub_queries, seed)
    # Group by policy_target to ensure gold/path pairings
    gold_pols = [p for p in candidates if p.get("policy_target") == "gold"]
    path_pols = [p for p in candidates if p.get("policy_target") == "path"]
    print(
        f"[seed {seed}] exhaustive: candidates={len(candidates)}, gold={len(gold_pols)}, path={len(path_pols)}, search_time={candidate_time:.6f}s (decomp reused)"
    )

    best = None
    best_pred = -float("inf")

    for g in gold_pols:
        qg = load_q_table_from_metadata(g)
        if qg is None:
            continue
        for p in path_pols:
            qp = load_q_table_from_metadata(p)
            if qp is None:
                continue
            # Build env for this seed (combined reward) to check expected Q-table size
            seed_val = int(seed) if seed else int(g.get("policy_seed", 0))
            env = init_gridworld_rand(
                seed=seed_val, reward_system="combined", grid_size=grid_size
            )
            expected_states = env.grid_length * env.grid_width

            # Validate Q-table sizes match environment
            if qg.shape[0] != expected_states:
                print(
                    f"    [seed {seed}] Skipping pair ({g.get('policy_name')}, {p.get('policy_name')}): "
                    f"Q-table size {qg.shape[0]} doesn't match environment size {expected_states}"
                )
                continue
            if qp.shape[0] != expected_states:
                print(
                    f"    [seed {seed}] Skipping pair ({g.get('policy_name')}, {p.get('policy_name')}): "
                    f"Q-table size {qp.shape[0]} doesn't match environment size {expected_states}"
                )
                continue

            q_combined = combine_q_tables(qg, qp)
            print(
                f"    [seed {seed}] training successor on pair ({g.get('policy_name')}, {p.get('policy_name')})"
            )
            embedding = embedding_from_qtable(
                env, q_combined, canonical_states, seed_val
            )
            pred = (
                float(
                    retriever.regressor_model.predict(
                        np.array(embedding).reshape(1, -1)
                    )[0]
                )
                if retriever.regressor_model
                else 0.0
            )
            if pred > best_pred:
                best_pred = pred
                best = {
                    "pair": (g, p),
                    "q_combined": q_combined,
                    "seed": seed_val,
                }

    elapsed = time.time() - start
    print(
        f"[seed {seed}] exhaustive total_time={elapsed:.6f}s (search + all pairs + SF training + regressor scoring, decomp reused)"
    )
    return best, elapsed


def load_canonical_states(states_folder: str = "states_16", canonical_states: int = 64):
    """
    Load canonical states, creating them if they don't exist.

    Args:
        states_folder: Name of the folder containing states (e.g., "states_16", "states_64")
        canonical_states: Total number of canonical states to collect (default: 64)
    """
    # Use folder-specific filename to avoid conflicts between different grid sizes
    canonical_file = f"data/canonical_states_{states_folder}.npy"

    if os.path.exists(canonical_file):
        loaded = np.load(canonical_file)
        # Verify dimensions match expected based on states_folder
        # Handle variations like states_16, states_16_1, states_64, etc.
        if states_folder.startswith("states_16"):
            expected_dim = 63  # 16x16 grids: 13 + 50 = 63
        elif states_folder.startswith("states_64"):
            expected_dim = 213  # 64x64 grids: 13 + 200 = 213
        else:
            # Default fallback
            expected_dim = 63
        if loaded.shape[0] > 0 and loaded.shape[1] == expected_dim:
            return loaded
        # If dimensions don't match, recreate
        print(
            f"Warning: {canonical_file} has wrong dimensions (got {loaded.shape[1]}, expected {expected_dim}), recreating..."
        )

    # Create new canonical states
    canonical_states_array = np.array(
        create_canonical_states(
            states_folder=states_folder, canonical_states=canonical_states
        )
    )
    # Save with folder-specific name
    os.makedirs("data", exist_ok=True)
    np.save(canonical_file, canonical_states_array)
    return canonical_states_array


def run_full_experiment(
    output_csv: str,
    query: str,
    seed_list: list[str] | None = None,
    states_folder: str = "states_64",
):
    retriever = PolicyRetriever(
        application_name="Grid World",
        available_actions=[
            "move up",
            "move right",
            "move right x2",
            "move down x2",
            "move diagonal",
        ],
    )

    # Determine grid size and canonical states count based on states_folder
    # Handle variations like states_16, states_16_1, states_64, etc.
    if states_folder.startswith("states_16"):
        grid_size = 16
        canonical_states_count = 64
    elif states_folder.startswith("states_64"):
        grid_size = 64
        canonical_states_count = 256
    else:
        # Default to 64x64
        print(
            f"Warning: Unknown states_folder '{states_folder}', defaulting to 64x64 grid"
        )
        grid_size = 64
        canonical_states_count = 256

    canonical_states = load_canonical_states(
        states_folder=states_folder, canonical_states=canonical_states_count
    )

    rows = []
    seeds = seed_list or sorted(
        {m.get("policy_seed") for m in retriever.vdb.metadata if m.get("policy_seed")}
    )

    # Decompose query once and reuse for all seeds
    print("Decomposing query (shared across all seeds)...")
    sub_queries, decomp_time = decompose_query_with_retry(
        retriever, query, max_attempts=5
    )
    print(
        f"Query decomposed into {len(sub_queries)} sub-queries. Reusing for all seeds.\n"
    )

    for seed in seeds:
        print(f"\n=== Running seed {seed} ===")
        # Scratch reward
        scratch_q_path = os.path.join(
            states_folder, "combined", f"seed_{seed}", "q_table_final.npy"
        )
        scratch_reward = None
        if os.path.exists(scratch_q_path):
            print(f"[seed {seed}] Loading scratch combined q_table: {scratch_q_path}")
            env = init_gridworld_rand(
                seed=int(seed), reward_system="combined", grid_size=grid_size
            )
            scratch_q = np.load(scratch_q_path)
            expected_states = env.grid_length * env.grid_width
            if scratch_q.shape[0] != expected_states:
                print(
                    f"[seed {seed}] scratch q_table size {scratch_q.shape[0]} doesn't match "
                    f"environment size {expected_states}. Skipping scratch evaluation."
                )
            else:
                scratch_reward = greedy_eval(env, scratch_q)
                print(f"[seed {seed}] scratch_reward: {scratch_reward}")
        else:
            print(f"[seed {seed}] scratch q_table not found at {scratch_q_path}")

        # Targeted (using shared decomposition)
        print(
            f"[seed {seed}] Starting targeted composition (shared decomposition + sim>0.7 + regressor rank)"
        )
        q_tgt, pair_tgt, dags_tgt, t_targeted = targeted_composition(
            retriever, seed, sub_queries, canonical_states
        )
        targeted_reward = None
        if q_tgt is not None:
            env = init_gridworld_rand(
                seed=int(seed), reward_system="combined", grid_size=grid_size
            )
            targeted_reward = greedy_eval(env, q_tgt)
            print(
                f"[seed {seed}] targeted_reward: {targeted_reward} (composition time {t_targeted:.6f}s)"
            )
        else:
            if t_targeted is not None:
                print(
                    f"[seed {seed}] targeted composition failed (composition time {t_targeted:.6f}s)"
                )
            else:
                print(f"[seed {seed}] targeted composition failed")

        # Exhaustive
        print(
            f"[seed {seed}] Starting exhaustive composition (shared decomposition + all gold/path pairs)"
        )
        best_exh, t_exhaustive = exhaustive_composition(
            retriever, seed, sub_queries, canonical_states, grid_size=grid_size
        )
        exhaustive_reward = None
        if best_exh is not None:
            env = init_gridworld_rand(
                seed=int(seed), reward_system="combined", grid_size=grid_size
            )
            exhaustive_reward = greedy_eval(env, best_exh["q_combined"])
            print(
                f"[seed {seed}] exhaustive_reward: {exhaustive_reward} (composition time {t_exhaustive:.6f}s)"
            )
        else:
            if t_exhaustive is not None:
                print(
                    f"[seed {seed}] exhaustive composition failed (composition time {t_exhaustive:.6f}s)"
                )
            else:
                print(f"[seed {seed}] exhaustive composition failed")

        rows.append(
            {
                "seed": seed,
                "scratch_reward": scratch_reward,
                "targeted_reward": targeted_reward,
                "exhaustive_reward": exhaustive_reward,
                "decomp_time": round(decomp_time, 6),
                "targeted_time": round(t_targeted, 6)
                if t_targeted is not None
                else None,
                "exhaustive_time": round(t_exhaustive, 6)
                if t_exhaustive is not None
                else None,
            }
        )

    df = pd.DataFrame(rows)
    os.makedirs(os.path.dirname(output_csv) or ".", exist_ok=True)
    df.to_csv(output_csv, index=False)
    print(f"Wrote results to {output_csv}")


def main():
    parser = argparse.ArgumentParser(
        description="Run full targeted vs exhaustive composition experiment."
    )
    parser.add_argument(
        "--output",
        type=str,
        default="full_experiment_results.csv",
        help="Path to output CSV",
    )
    parser.add_argument(
        "--seeds",
        type=str,
        nargs="*",
        help="Optional list of seeds to run (e.g., 0000 0001)",
    )
    parser.add_argument(
        "--query",
        type=str,
        required=True,
        help="Natural language query to decompose and search (used for both targeted and exhaustive)",
    )
    parser.add_argument(
        "--states-folder",
        type=str,
        default="states_16",
        help="States folder to use (states_16 for 16x16 grids, states_64 for 64x64 grids)",
    )
    args = parser.parse_args()
    run_full_experiment(
        args.output, args.query, args.seeds, states_folder=args.states_folder
    )


if __name__ == "__main__":
    main()
